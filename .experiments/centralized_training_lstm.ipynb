{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "module_path = '/home/sbose/time-series-forecasting-federation'\n",
    "FOLDER_NAME = 'centralized_results'\n",
    "        \n",
    "# configure model and other stuff\n",
    "lookahead = 4\n",
    "dtype = torch.float32\n",
    "device = 'cuda'\n",
    "num_clients = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingent imports\n",
    "sys.path.insert(0,module_path)\n",
    "from files_for_appfl.comstock_dataloader import get_comstock_shared_norm, get_comstock_range\n",
    "from files_for_appfl.loss import MSELoss\n",
    "from files_for_appfl.loss_last import MSELoss as MSELoss2\n",
    "from files_for_appfl.metric import mape\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "from models.DARNN.DARNN2 import DARNN\n",
    "from models.TRANSFORMER.Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero weight init\n",
    "def zero_weights(model):\n",
    "    for param in model.parameters():\n",
    "        param.data.zero_()\n",
    "        \n",
    "# function to zero the weights for initialization\n",
    "def normal_weights(model):\n",
    "    for param in model.parameters():\n",
    "        param.data.normal_()\n",
    "        \n",
    "# function to calculate norm of gradients\n",
    "def calculate_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for param in model.parameters():\n",
    "        param_norm = param.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master function for training\n",
    "def train_and_test_lstm(\n",
    "optim_name, # pass as the name containe in a string\n",
    "custom_str = 'DARNN, FullFeatureSet, LongTrain',\n",
    "normalize = 'True',\n",
    "display_time_idx = 500,\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "seed = 42,\n",
    "BS = 32,\n",
    "steps = 1000,\n",
    "clip_grad = np.inf,\n",
    "ntype = 'z',\n",
    "hidden_size = 20,\n",
    "lr = 1e-5,\n",
    "test_every = 100,\n",
    "lookback = 12,\n",
    "num_lstm_layer = 12\n",
    "):\n",
    "    \n",
    "    # master function to train on data and produce output on test set \n",
    "    model_kwargs = {\n",
    "        'x_size': 6,\n",
    "        'y_size': 1,\n",
    "        'u_size': 2,\n",
    "        's_size': 7,\n",
    "        'lookback': lookback,\n",
    "        'lookahead': lookahead,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_lstm_layers': 2,\n",
    "        'dtype' : dtype\n",
    "    }\n",
    "    model = nn.DataParallel(DARNN(**model_kwargs))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optim = eval(optim_name)(model.parameters(), **{'lr':lr})\n",
    "    loss_fn = MSELoss(ntype)\n",
    "    loss_fn_to_report = MSELoss2(ntype)\n",
    "    \n",
    "    # get and combine datasets\n",
    "    _, train_set, test_set = get_comstock_range(\n",
    "        end_bldg_idx=num_clients,\n",
    "        lookback = lookback,\n",
    "        lookahead = lookahead,\n",
    "        dtype = dtype,\n",
    "        normalize = normalize,\n",
    "        normalize_type=ntype\n",
    "    )\n",
    "    train_set, test_set = ConcatDataset(train_set), ConcatDataset(test_set)\n",
    "    torch.manual_seed(seed)\n",
    "    train_loader = DataLoader(train_set, batch_size=BS, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=8096, shuffle=True)\n",
    "\n",
    "    loss_record, mape_record, norm_record = [], [], []\n",
    "    elapsed = 0\n",
    "    normal_weights(model) # actually initializes to normal, doesnt zero   \n",
    "    for inp, lab in (t:=tqdm(itertools.cycle(train_loader))):\n",
    "    \n",
    "        inp, lab = inp.to(device), lab.to(device)\n",
    "        pred = model(inp)\n",
    "        loss = loss_fn(lab,pred)\n",
    "        loss_to_report = loss_fn_to_report(lab,pred)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        if not np.isinf(clip_grad) and clip_grad > 0:\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        norm_record.append(calculate_gradient_norm(model))\n",
    "        optim.step()\n",
    "        # scheduler.step()    \n",
    "        loss_record.append(loss_to_report.item())\n",
    "        elapsed += 1\n",
    "        \n",
    "        t.set_description(f\"On experiment {custom_str}, step {elapsed}, loss is {loss.item()}.\")\n",
    "        \n",
    "        if elapsed % test_every == 0:\n",
    "            mapes = []\n",
    "            for inp,lab in test_loader:\n",
    "                inp = inp.to(device)\n",
    "                with torch.no_grad():\n",
    "                    pred = model(inp)\n",
    "                mapes.append(mape(lab.to('cpu').numpy(),pred.to('cpu').numpy(),normalization_type=ntype))\n",
    "            metric = np.mean(np.array(mapes))\n",
    "            mape_record.append(metric)\n",
    "            print(f\"On step {elapsed}, MAPE error is {metric} percent.\")\n",
    "            \n",
    "        if elapsed == steps:\n",
    "            break\n",
    "    \n",
    "    # plotting here\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20,4))\n",
    "    # plot losses\n",
    "    loss_record = np.array(loss_record)\n",
    "    axs[0].plot(np.arange(1,loss_record.size+1),np.array(loss_record))\n",
    "    axs[0].set_xlim(1,loss_record.size)\n",
    "    axs[0].set_xlabel('Steps')\n",
    "    axs[0].set_ylabel('MSE Loss')\n",
    "    axs[0].set_title(f'Train Loss')\n",
    "    axs[0].set_yscale('log')\n",
    "    # plot norms\n",
    "    norm_record = np.array(norm_record)\n",
    "    axs[1].plot(np.arange(1,norm_record.size+1),np.array(norm_record))\n",
    "    axs[1].set_xlim(1,norm_record.size)\n",
    "    axs[1].set_xlabel('Steps')\n",
    "    axs[1].set_ylabel('L2 Norm')\n",
    "    axs[1].set_title(f'Gradient Norm clip: {clip_grad}')\n",
    "    axs[1].set_yscale('log')\n",
    "    # plot MAPEs\n",
    "    mape_record = np.array(mape_record)\n",
    "    axs[2].plot(np.arange(1,mape_record.size+1),np.array(mape_record),'ko')\n",
    "    axs[2].set_xlim(1,mape_record.size)\n",
    "    axs[2].set_xlabel(f'Steps x{test_every}')\n",
    "    axs[2].set_ylabel(f'MAPE')\n",
    "    axs[2].set_title(f'Test set.')\n",
    "    # plot the test sets\n",
    "    inputs, outputs = [], []\n",
    "    for idx in range(display_time_idx):\n",
    "        itm = test_set.__getitem__(idx)\n",
    "        inputs.append(itm[0])\n",
    "        outputs.append(itm[1].numpy())\n",
    "    batched_input = torch.stack(inputs).to(dtype).to(device)\n",
    "    with torch.no_grad():   \n",
    "        batched_output = model(batched_input).to('cpu').numpy()\n",
    "    preds = list(batched_output)\n",
    "    plot_gt, plot_pred = [], []\n",
    "    for idx in range(display_time_idx):\n",
    "        minval, maxval = outputs[idx][-1,1], outputs[idx][-1,2]\n",
    "        if ntype == 'minmax':\n",
    "            # minmax\n",
    "            plot_gt.append((outputs[idx][-1,0]-minval)/(maxval-minval))\n",
    "        else:\n",
    "            # z normalization\n",
    "            plot_gt.append((outputs[idx][-1,0]-minval)/maxval)\n",
    "        plot_pred.append(preds[idx][-1,0])\n",
    "    plot_gt, plot_pred = np.array(plot_gt), np.array(plot_pred)\n",
    "    axs[3].plot(np.arange(1,plot_gt.size+1),plot_gt,label='ground truth')\n",
    "    axs[3].plot(np.arange(1,plot_pred.size+1),plot_pred,label='prediction')\n",
    "    axs[3].set_xlim(1,plot_pred.size)\n",
    "    axs[3].set_xlabel('Time index')\n",
    "    axs[3].set_ylabel('kWh')\n",
    "    axs[3].legend()\n",
    "    axs[3].set_title(f'Reconstruction')\n",
    "    \n",
    "    optdict = {\n",
    "        'torch.optim.SGD': 'sgd',\n",
    "        'torch.optim.Adam': 'adam'\n",
    "    }\n",
    "    \n",
    "    plt.suptitle(f'Optim={optdict[optim_name]}, BS={BS}, lr={lr}, clip={clip_grad}')\n",
    "    \n",
    "    plt.savefig(f'/home/sbose/{FOLDER_NAME}/{optdict[optim_name]}_BS_{BS}_lr_{lr}_lookback_{lookback}_lstm_{steps}.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # torch.save(model.state_dict(),f'/home/sbose/{FOLDER_NAME}/{optdict[optim_name]}_BS_{BS}_lr_{lr}_clip_{clip_grad}.pth')\n",
    "    np.savez_compressed(f'/home/sbose/{FOLDER_NAME}/{optdict[optim_name]}_BS_{BS}_lr_{lr}_lookback_{lookback}_lstm_{steps}.npz',loss_record=loss_record, mape_record=mape_record,norm_record=norm_record,preds=preds,outputs=outputs)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "On experiment Transformer, step 334, loss is 38.295074462890625.: : 334it [01:45,  3.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/sbose/\u001b[39m\u001b[39m{\u001b[39;00mFOLDER_NAME\u001b[39m}\u001b[39;00m\u001b[39m/done.txt\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m l, b, c, o, lb, stp \u001b[39min\u001b[39;00m configs:\n\u001b[0;32m---> 12\u001b[0m     train_and_test_lstm(\n\u001b[1;32m     13\u001b[0m         o, \u001b[39m# pass as the name containe in a string\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mTransformer\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m         normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     16\u001b[0m         display_time_idx\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m         ntype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mz\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m         hidden_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m         BS \u001b[39m=\u001b[39;49m b,\n\u001b[1;32m     20\u001b[0m         lr \u001b[39m=\u001b[39;49m l,\n\u001b[1;32m     21\u001b[0m         steps\u001b[39m=\u001b[39;49mstp,\n\u001b[1;32m     22\u001b[0m         test_every \u001b[39m=\u001b[39;49m \u001b[39m6000\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m         clip_grad\u001b[39m=\u001b[39;49m c,\n\u001b[1;32m     24\u001b[0m         lookback \u001b[39m=\u001b[39;49m lb\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinished opt=\u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m}\u001b[39;00m\u001b[39m, lr=\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m, BS=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, Clip=\u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     file\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mtrain_and_test_lstm\u001b[0;34m(optim_name, custom_str, normalize, display_time_idx, device, seed, BS, steps, clip_grad, ntype, hidden_size, lr, test_every, lookback, num_lstm_layer)\u001b[0m\n\u001b[1;32m     61\u001b[0m loss_to_report \u001b[39m=\u001b[39m loss_fn_to_report(lab,pred)\n\u001b[1;32m     62\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 63\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misinf(clip_grad) \u001b[39mand\u001b[39;00m clip_grad \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[39m# clip gradients\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), clip_grad)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    [1e-4, 128, 100, 'torch.optim.SGD', 12, 20000],\n",
    "    [1e-5, 128, 100, 'torch.optim.SGD', 12, 20000],\n",
    "    [1e-4, 128, 100, 'torch.optim.SGD', 12, 100000]\n",
    "]\n",
    "\n",
    "os.makedirs(f'/home/sbose/{FOLDER_NAME}',exist_ok=True)\n",
    "file = open(f'/home/sbose/{FOLDER_NAME}/done.txt','a')\n",
    "\n",
    "for l, b, c, o, lb, stp in configs:\n",
    "    \n",
    "    train_and_test_lstm(\n",
    "        o, # pass as the name containe in a string\n",
    "        'LSTM - DARNN',\n",
    "        normalize=True,\n",
    "        display_time_idx=250,\n",
    "        ntype='z',\n",
    "        hidden_size=128,\n",
    "        BS = b,\n",
    "        lr = l,\n",
    "        steps=stp,\n",
    "        test_every = 6000,\n",
    "        clip_grad= c,\n",
    "        lookback = lb\n",
    "    )\n",
    "    print(f\"Finished opt={o}, lr={l}, BS={b}, Clip={c}.\")\n",
    "    file.write(f\"'{o}',{l},{b},{c}\\n\")\n",
    "    \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
