{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 19 13:28:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   21C    P0              50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   21C    P0              52W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# infer GPU in use\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "computer = 'argonne' # 'ucsc' or 'argonne'\n",
    "\n",
    "# configure filepaths according to which computer we are on\n",
    "\n",
    "if computer == 'ucsc':\n",
    "    data_file = '/home/exx/shourya/comstock_data/G4601010_data.npz'\n",
    "    weather_file = '/home/exx/shourya/comstock_data/G4601010_weather.npz'\n",
    "    module_path = '/home/exx/shourya/time-series-forecasting-federation'\n",
    "else:\n",
    "    data_file = '/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_data.npz'\n",
    "    weather_file = '/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_weather.npz'\n",
    "    module_path = '/home/sbose/time-series-forecasting-federation'\n",
    "\n",
    "# in this notebook, we will analyse the data files and try to see how we can make a custom data loader for the same.\n",
    "\n",
    "data_y_s = np.load(data_file)\n",
    "data_x_u = np.load(weather_file)\n",
    "\n",
    "# function to calculate model sizes\n",
    "\n",
    "def model_size_in_mb(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    size_in_mb = param_size / (1024 ** 2)\n",
    "    return f'{size_in_mb:.6f} MB'\n",
    "\n",
    "# function to save model to disk and calculate its size\n",
    "\n",
    "def save_and_measure_model(model):\n",
    "    # Save state_dict\n",
    "    torch.save(model.state_dict(), 'model_state.pth')\n",
    "\n",
    "    # Measure file size\n",
    "    file_size = os.path.getsize('model_state.pth') / (1024 * 1024)\n",
    "\n",
    "    # Delete the file\n",
    "    os.remove('model_state.pth')\n",
    "\n",
    "    return f'{file_size:.6f} MB'\n",
    "    \n",
    "# base data type\n",
    "base_type = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "with_mean: True\n",
      "with_std: True\n",
      "copy: True\n",
      "n_features_in_: 1\n",
      "n_samples_seen_: 7\n",
      "mean_: [11273.82960991]\n",
      "var_: [6.77648757e+08]\n",
      "scale_: [26031.68755852]\n",
      "\n",
      "\n",
      "\n",
      "with_mean: True\n",
      "with_std: True\n",
      "copy: True\n",
      "n_features_in_: 6\n",
      "n_samples_seen_: 28032\n",
      "mean_: [  8.62998778   4.53433665 182.47452911  67.67972317 202.4050371\n",
      " 181.32121793]\n",
      "var_: [1.90457134e+02 7.10934475e+00 6.77019366e+04 8.74746816e+03\n",
      " 9.36539076e+04 1.00097295e+04]\n",
      "scale_: [ 13.80062079   2.66633545 260.19595808  93.52790045 306.02925941\n",
      " 100.04863583]\n",
      "\n",
      "\n",
      "\n",
      "with_mean: True\n",
      "with_std: True\n",
      "copy: True\n",
      "n_features_in_: 2\n",
      "n_samples_seen_: 28032\n",
      "mean_: [47.5         2.98305508]\n",
      "var_: [767.91666667   3.98241121]\n",
      "scale_: [27.71130936  1.99559796]\n",
      "\n",
      "\n",
      "\n",
      "with_mean: True\n",
      "with_std: True\n",
      "copy: True\n",
      "n_features_in_: 1\n",
      "n_samples_seen_: 7\n",
      "mean_: [11273.82960991]\n",
      "var_: [6.77648757e+08]\n",
      "scale_: [26031.68755852]\n"
     ]
    }
   ],
   "source": [
    "# We now test out our dataset with california data\n",
    "\n",
    "sys.path.insert(0,module_path)\n",
    "from models.LFDataset import LFDataset\n",
    "\n",
    "# create dataset\n",
    "CA_dset = LFDataset(\n",
    "    data_y_s = data_y_s,\n",
    "    data_x_u = data_x_u,\n",
    "    lookback = 100,\n",
    "    lookahead = 4,\n",
    "    client_idx = 0,\n",
    "    idx_x = [0,1,2,3,4,5],\n",
    "    idx_u = [6,7],\n",
    "    dtype = base_type,\n",
    "    normalize=True,\n",
    "    normalize_type='z'\n",
    ")\n",
    "\n",
    "# load into dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "CA_dataloader = DataLoader(CA_dset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LFDataset._transform() missing 1 required positional argument: 'y_tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test out the shape of the dataloader outputs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m cidx, (a,b) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(CA_dataloader):\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of \u001b[39m\u001b[39m{\u001b[39;00mcidx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39mth item in a is \u001b[39m\u001b[39m{\u001b[39;00ma\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of \u001b[39m\u001b[39m{\u001b[39;00mcidx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39mth item in b is \u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/time-series-forecasting-federation/models/LFDataset.py:98\u001b[0m, in \u001b[0;36mLFDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     95\u001b[0m y_all_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload[idx\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookback:idx\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookback\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookahead][:,\u001b[39mNone\u001b[39;00m], dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)  \n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize:\n\u001b[0;32m---> 98\u001b[0m     y_past, x_past, u_past, u_future, s_past, y_all_target_scaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(y_past, x_past, u_past, u_future, s_past)  \n\u001b[1;32m    100\u001b[0m     \u001b[39m# calculate factors\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mminmax\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: LFDataset._transform() missing 1 required positional argument: 'y_tar'"
     ]
    }
   ],
   "source": [
    "# Test out the shape of the dataloader outputs\n",
    "\n",
    "for cidx, (a,b) in enumerate(CA_dataloader):\n",
    "    print(f\"Shape of {cidx+1}th item in a is {a.shape}.\")\n",
    "    print(f\"Shape of {cidx+1}th item in b is {b.shape}.\")\n",
    "    break\n",
    "\n",
    "l = a[0]\n",
    "print(l.shape)\n",
    "def save_array_to_csv(array, column_titles, filename):\n",
    "    assert array.shape[1] == len(column_titles), \"Column titles length must match array's second dimension\"\n",
    "    header = ','.join(column_titles)\n",
    "    np.savetxt(filename, array, delimiter=',', header=header, comments='')\n",
    "names = [\n",
    "    'load',\n",
    "    'dry bulb temp',\n",
    "    'wind speed',\n",
    "    'global horizontal radiation',\n",
    "    'diffuse horizontal radiation',\n",
    "    'direct normal radiation',\n",
    "    'wind direction',\n",
    "    'time index',\n",
    "    'weekday index',\n",
    "    'floor area',\n",
    "    'wall area',\n",
    "    'window area',\n",
    "    'number of spaces',\n",
    "    'number of zones',\n",
    "    'number of surfaces',\n",
    "    'cooling equipment capacity'\n",
    "]\n",
    "save_array_to_csv(l[:,:16],names,'client0_100rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that relative imports from the git repository can always be found\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,module_path)\n",
    "\n",
    "# kwargs for all models\n",
    "model_kwargs = {\n",
    "    'x_size': 6,\n",
    "    'y_size': 1,\n",
    "    'u_size': 2,\n",
    "    's_size': 7,\n",
    "    'lookback': 12,\n",
    "    'lookahead': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM AR output is (32, 4, 1)\n",
      "\n",
      "LSTM AR, torch.float32, theoretical: 0.167164 MB\n",
      "LSTM AR, torch.float32, state_dict on disk: 0.167164 MB\n",
      "LSTM AR, torch.float16, theoretical: 0.083582 MB\n",
      "LSTM AR, torch.float16, state_dict on disk: 0.083582 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM autoregressive version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "\n",
    "model = LSTMAR(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'LSTM AR'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LSTMAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DARNN output is (32, 4, 1)\n",
      "\n",
      "DARNN, torch.float32, theoretical: 0.057205 MB\n",
      "DARNN, torch.float32, state_dict on disk: 0.057205 MB\n",
      "DARNN, torch.float16, theoretical: 0.028603 MB\n",
      "DARNN, torch.float16, state_dict on disk: 0.028603 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out DARNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.DARNN.DARNN import DARNN\n",
    "\n",
    "model = DARNN(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'DARNN'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = DARNN(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformer output is (32, 4, 1)\n",
      "\n",
      "Transformer, torch.float32, theoretical: 2.116947 MB\n",
      "Transformer, torch.float32, state_dict on disk: 2.116947 MB\n",
      "Transformer, torch.float16, theoretical: 1.058474 MB\n",
      "Transformer, torch.float16, state_dict on disk: 1.058474 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Transformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.TRANSFORMER.Transformer import Transformer\n",
    "\n",
    "model = Transformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Transformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Transformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LogTrans output is (32, 4, 1)\n",
      "\n",
      "LogTrans, torch.float32, theoretical: 1.419682 MB\n",
      "LogTrans, torch.float32, state_dict on disk: 1.419682 MB\n",
      "LogTrans, torch.float16, theoretical: 0.709841 MB\n",
      "LogTrans, torch.float16, state_dict on disk: 0.709841 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Logtrans\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LOGTRANS.LogTrans import LogTrans\n",
    "model = LogTrans(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'LogTrans'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LogTrans(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Informer output is (32, 4, 1)\n",
      "\n",
      "Informer, torch.float32, theoretical: 1.467289 MB\n",
      "Informer, torch.float32, state_dict on disk: 1.467289 MB\n",
      "Informer, torch.float16, theoretical: 0.733644 MB\n",
      "Informer, torch.float16, state_dict on disk: 0.733644 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Informer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.INFORMER.Informer import Informer\n",
    "model = Informer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Informer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Informer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Autoformer output is (32, 4, 1)\n",
      "\n",
      "Autoformer, torch.float32, theoretical: 1.407627 MB\n",
      "Autoformer, torch.float32, state_dict on disk: 1.407627 MB\n",
      "\n",
      "NOTICE: Autoformer does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Autoformer, torch.float64, theoretical: 2.815254 MB\n",
      "Autoformer, torch.float64, state_dict on disk: 2.815254 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Autoformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.AUTOFORMER.Autoformer import Autoformer\n",
    "model = Autoformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Autoformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = Autoformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Fedformer Wavelet output is (32, 4, 1)\n",
      "\n",
      "Fedformer Wavelet, torch.float32, theoretical: 770.922871 MB\n",
      "Fedformer Wavelet, torch.float32, state_dict on disk: 770.922871 MB\n",
      "\n",
      "NOTICE: Fedformer Wavelet does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Fedformer Wavelet, torch.float64, theoretical: 1541.845467 MB\n",
      "Fedformer Wavelet, torch.float64, state_dict on disk: 1541.845467 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Fedformer Wavelet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.FEDFORMER.FedformerWavelet import FedformerWavelet\n",
    "model = FedformerWavelet(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Fedformer Wavelet'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = FedformerWavelet(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Fedformer Fourier output is (32, 4, 1)\n",
      "\n",
      "Fedformer Fourier, torch.float32, theoretical: 1.468189 MB\n",
      "Fedformer Fourier, torch.float32, state_dict on disk: 1.468189 MB\n",
      "\n",
      "NOTICE: Fedformer Fourier does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Fedformer Fourier, torch.float64, theoretical: 2.936378 MB\n",
      "Fedformer Fourier, torch.float64, state_dict on disk: 2.936378 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Fedformer Fourier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.FEDFORMER.FedformerFourier import FedformerFourier\n",
    "model = FedformerFourier(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Fedformer Fourier'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = FedformerFourier(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Crossformer output is (32, 4, 1)\n",
      "\n",
      "Crossformer, torch.float32, theoretical: 3.905296 MB\n",
      "Crossformer, torch.float32, state_dict on disk: 3.905296 MB\n",
      "Crossformer, torch.float16, theoretical: 1.952648 MB\n",
      "Crossformer, torch.float16, state_dict on disk: 1.952648 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Crossformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.CROSSFORMER.Crossformer import Crossformer\n",
    "model = Crossformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Crossformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Crossformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mLSTM output is (32, 4, 1)\n",
      "\n",
      "mLSTM, torch.float32, theoretical: 1.063274 MB\n",
      "mLSTM, torch.float32, state_dict on disk: 1.063274 MB\n",
      "mLSTM, torch.float16, theoretical: 0.531637 MB\n",
      "mLSTM, torch.float16, state_dict on disk: 0.531637 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Crossformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.XLSTM.mLSTM import mLSTM\n",
    "model = mLSTM(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'mLSTM'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = mLSTM(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
