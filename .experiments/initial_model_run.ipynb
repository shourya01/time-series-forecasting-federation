{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 20 16:07:02 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              75W / 400W |  13619MiB / 40960MiB |     82%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   42C    P0             234W / 400W |   2602MiB / 40960MiB |     98%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   20C    P0              51W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   21C    P0              52W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   42C    P0             200W / 400W |   2042MiB / 40960MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   60C    P0             285W / 400W |    986MiB / 40960MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   24C    P0              56W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              52W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    231684      C   ...iconda3/envs/hydrosm_pyg/bin/python     1008MiB |\n",
      "|    0   N/A  N/A    254152      C   .../sbose/.conda/envs/appfl/bin/python     1810MiB |\n",
      "|    0   N/A  N/A    263597      C   python                                    10250MiB |\n",
      "|    0   N/A  N/A   2823316      C   ...iconda3/envs/hydrosm_pyg/bin/python      526MiB |\n",
      "|    1   N/A  N/A    181386      C   ...cai/mumax3.10_linux_cuda11.0/mumax3      774MiB |\n",
      "|    1   N/A  N/A    254152      C   .../sbose/.conda/envs/appfl/bin/python     1814MiB |\n",
      "|    4   N/A  N/A    205803      C   ...cai/mumax3.10_linux_cuda11.0/mumax3     2034MiB |\n",
      "|    5   N/A  N/A   1321037      C   ...cai/mumax3.10_linux_cuda11.0/mumax3      978MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# infer GPU in use\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "computer = 'argonne' # 'ucsc' or 'argonne'\n",
    "\n",
    "# configure filepaths according to which computer we are on\n",
    "\n",
    "if computer == 'ucsc':\n",
    "    data_file = '/home/exx/shourya/comstock_data/G4601010_data.npz'\n",
    "    weather_file = '/home/exx/shourya/comstock_data/G4601010_weather.npz'\n",
    "    module_path = '/home/exx/shourya/time-series-forecasting-federation'\n",
    "else:\n",
    "    data_file = '/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_data.npz'\n",
    "    weather_file = '/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_weather.npz'\n",
    "    module_path = '/home/sbose/time-series-forecasting-federation'\n",
    "\n",
    "# in this notebook, we will analyse the data files and try to see how we can make a custom data loader for the same.\n",
    "\n",
    "data_y_s = np.load(data_file)\n",
    "data_x_u = np.load(weather_file)\n",
    "\n",
    "# function to calculate model sizes\n",
    "\n",
    "def model_size_in_mb(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    size_in_mb = param_size / (1024 ** 2)\n",
    "    return f'{size_in_mb:.6f} MB'\n",
    "\n",
    "# function to save model to disk and calculate its size\n",
    "\n",
    "def save_and_measure_model(model):\n",
    "    # Save state_dict\n",
    "    torch.save(model.state_dict(), 'model_state.pth')\n",
    "\n",
    "    # Measure file size\n",
    "    file_size = os.path.getsize('model_state.pth') / (1024 * 1024)\n",
    "\n",
    "    # Delete the file\n",
    "    os.remove('model_state.pth')\n",
    "\n",
    "    return f'{file_size:.6f} MB'\n",
    "    \n",
    "# base data type\n",
    "base_type = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now test out our dataset with california data\n",
    "\n",
    "sys.path.insert(0,module_path)\n",
    "from models.LFDataset import LFDataset\n",
    "\n",
    "# create dataset\n",
    "CA_dset = LFDataset(\n",
    "    data_y_s = data_y_s,\n",
    "    data_x_u = data_x_u,\n",
    "    lookback = 100,\n",
    "    lookahead = 4,\n",
    "    client_idx = 0,\n",
    "    idx_x = [0,1,2,3,4,5],\n",
    "    idx_u = [6,7],\n",
    "    dtype = base_type,\n",
    "    # normalize=True,\n",
    "    normalize_type=None\n",
    ")\n",
    "\n",
    "# load into dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "CA_dataloader = DataLoader(CA_dset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 1th item in a is torch.Size([32, 100, 19]).\n",
      "Shape of 1th item in b is torch.Size([32, 4, 3]).\n",
      "torch.Size([100, 19])\n"
     ]
    }
   ],
   "source": [
    "# Test out the shape of the dataloader outputs\n",
    "\n",
    "for cidx, (a,b) in enumerate(CA_dataloader):\n",
    "    print(f\"Shape of {cidx+1}th item in a is {a.shape}.\")\n",
    "    print(f\"Shape of {cidx+1}th item in b is {b.shape}.\")\n",
    "    break\n",
    "\n",
    "l = a[0]\n",
    "print(l.shape)\n",
    "def save_array_to_csv(array, column_titles, filename):\n",
    "    assert array.shape[1] == len(column_titles), \"Column titles length must match array's second dimension\"\n",
    "    header = ','.join(column_titles)\n",
    "    np.savetxt(filename, array, delimiter=',', header=header, comments='')\n",
    "names = [\n",
    "    'load',\n",
    "    'dry bulb temp',\n",
    "    'wind speed',\n",
    "    'global horizontal radiation',\n",
    "    'diffuse horizontal radiation',\n",
    "    'direct normal radiation',\n",
    "    'wind direction',\n",
    "    'time index',\n",
    "    'weekday index',\n",
    "    'floor area',\n",
    "    'wall area',\n",
    "    'window area',\n",
    "    'number of spaces',\n",
    "    'number of zones',\n",
    "    'number of surfaces',\n",
    "    'cooling equipment capacity'\n",
    "]\n",
    "save_array_to_csv(l[:,:16],names,'client0_100rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that relative imports from the git repository can always be found\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,module_path)\n",
    "\n",
    "# kwargs for all models\n",
    "model_kwargs = {\n",
    "    'x_size': 6,\n",
    "    'y_size': 1,\n",
    "    'u_size': 2,\n",
    "    's_size': 7,\n",
    "    'lookback': 100,\n",
    "    'lookahead': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM AR output is (32, 4, 1)\n",
      "\n",
      "LSTM AR, torch.float32, theoretical: 0.167164 MB\n",
      "LSTM AR, torch.float32, state_dict on disk: 0.167164 MB\n",
      "LSTM AR, torch.float16, theoretical: 0.083582 MB\n",
      "LSTM AR, torch.float16, state_dict on disk: 0.083582 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM autoregressive version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "\n",
    "model = LSTMAR(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'LSTM AR'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LSTMAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DARNN output is (32, 4, 1)\n",
      "\n",
      "DARNN, torch.float32, theoretical: 0.108566 MB\n",
      "DARNN, torch.float32, state_dict on disk: 0.108566 MB\n",
      "DARNN, torch.float16, theoretical: 0.054283 MB\n",
      "DARNN, torch.float16, state_dict on disk: 0.054283 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out DARNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.DARNN.DARNN import DARNN\n",
    "\n",
    "model = DARNN(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'DARNN'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = DARNN(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DARNN2 output is (32, 4, 1)\n",
      "\n",
      "DARNN2, torch.float32, theoretical: 0.103092 MB\n",
      "DARNN2, torch.float32, state_dict on disk: 0.103092 MB\n",
      "DARNN2, torch.float16, theoretical: 0.051546 MB\n",
      "DARNN2, torch.float16, state_dict on disk: 0.051546 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out DARNN2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.DARNN.DARNN2 import DARNN\n",
    "\n",
    "model = DARNN(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'DARNN2'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = DARNN(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformer output is (32, 4, 1)\n",
      "\n",
      "Transformer, torch.float32, theoretical: 2.116947 MB\n",
      "Transformer, torch.float32, state_dict on disk: 2.116947 MB\n",
      "Transformer, torch.float16, theoretical: 1.058474 MB\n",
      "Transformer, torch.float16, state_dict on disk: 1.058474 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Transformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.TRANSFORMER.Transformer import Transformer\n",
    "\n",
    "model = Transformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Transformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Transformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LogTrans output is (32, 4, 1)\n",
      "\n",
      "LogTrans, torch.float32, theoretical: 1.419682 MB\n",
      "LogTrans, torch.float32, state_dict on disk: 1.419682 MB\n",
      "LogTrans, torch.float16, theoretical: 0.709841 MB\n",
      "LogTrans, torch.float16, state_dict on disk: 0.709841 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Logtrans\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LOGTRANS.LogTrans import LogTrans\n",
    "model = LogTrans(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'LogTrans'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LogTrans(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Informer output is (32, 4, 1)\n",
      "\n",
      "Informer, torch.float32, theoretical: 1.467289 MB\n",
      "Informer, torch.float32, state_dict on disk: 1.467289 MB\n",
      "Informer, torch.float16, theoretical: 0.733644 MB\n",
      "Informer, torch.float16, state_dict on disk: 0.733644 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Informer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.INFORMER.Informer import Informer\n",
    "model = Informer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Informer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Informer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Autoformer output is (32, 4, 1)\n",
      "\n",
      "Autoformer, torch.float32, theoretical: 1.407627 MB\n",
      "Autoformer, torch.float32, state_dict on disk: 1.407627 MB\n",
      "\n",
      "NOTICE: Autoformer does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Autoformer, torch.float64, theoretical: 2.815254 MB\n",
      "Autoformer, torch.float64, state_dict on disk: 2.815254 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Autoformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.AUTOFORMER.Autoformer import Autoformer\n",
    "model = Autoformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Autoformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = Autoformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Fedformer Wavelet output is (32, 4, 1)\n",
      "\n",
      "Fedformer Wavelet, torch.float32, theoretical: 770.922871 MB\n",
      "Fedformer Wavelet, torch.float32, state_dict on disk: 770.922871 MB\n",
      "\n",
      "NOTICE: Fedformer Wavelet does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Fedformer Wavelet, torch.float64, theoretical: 1541.845467 MB\n",
      "Fedformer Wavelet, torch.float64, state_dict on disk: 1541.845467 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Fedformer Wavelet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.FEDFORMER.FedformerWavelet import FedformerWavelet\n",
    "model = FedformerWavelet(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Fedformer Wavelet'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = FedformerWavelet(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Fedformer Fourier output is (32, 4, 1)\n",
      "\n",
      "Fedformer Fourier, torch.float32, theoretical: 1.733814 MB\n",
      "Fedformer Fourier, torch.float32, state_dict on disk: 1.733814 MB\n",
      "\n",
      "NOTICE: Fedformer Fourier does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Fedformer Fourier, torch.float64, theoretical: 3.467628 MB\n",
      "Fedformer Fourier, torch.float64, state_dict on disk: 3.467628 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Fedformer Fourier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.FEDFORMER.FedformerFourier import FedformerFourier\n",
    "model = FedformerFourier(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Fedformer Fourier'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = FedformerFourier(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Crossformer output is (32, 4, 1)\n",
      "\n",
      "Crossformer, torch.float32, theoretical: 3.909454 MB\n",
      "Crossformer, torch.float32, state_dict on disk: 3.909454 MB\n",
      "Crossformer, torch.float16, theoretical: 1.954727 MB\n",
      "Crossformer, torch.float16, state_dict on disk: 1.954727 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Crossformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.CROSSFORMER.Crossformer import Crossformer\n",
    "model = Crossformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Crossformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Crossformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mLSTM output is (32, 4, 1)\n",
      "\n",
      "mLSTM, torch.float32, theoretical: 1.063274 MB\n",
      "mLSTM, torch.float32, state_dict on disk: 1.063274 MB\n",
      "mLSTM, torch.float16, theoretical: 0.531637 MB\n",
      "mLSTM, torch.float16, state_dict on disk: 0.531637 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Crossformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.XLSTM.mLSTM import mLSTM\n",
    "model = mLSTM(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'mLSTM'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = mLSTM(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
