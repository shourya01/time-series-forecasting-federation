{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "module_path = '/home/sbose/time-series-forecasting-federation'\n",
    "FOLDER_NAME = 'centralized_results'\n",
    "        \n",
    "# configure model and other stuff\n",
    "lookahead = 4\n",
    "dtype = torch.float32\n",
    "device = 'cuda'\n",
    "num_clients = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingent imports\n",
    "sys.path.insert(0,module_path)\n",
    "from files_for_appfl.comstock_dataloader import get_comstock_shared_norm, get_comstock_range\n",
    "from files_for_appfl.loss_last import MSELoss\n",
    "from files_for_appfl.metric import mape\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "from models.DARNN.DARNN import DARNN\n",
    "from models.TRANSFORMER.Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero weight init\n",
    "def zero_weights(model):\n",
    "    for param in model.parameters():\n",
    "        param.data.zero_()\n",
    "        \n",
    "# function to zero the weights for initialization\n",
    "def normal_weights(model):\n",
    "    for param in model.parameters():\n",
    "        param.data.normal_()\n",
    "        \n",
    "# function to calculate norm of gradients\n",
    "def calculate_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for param in model.parameters():\n",
    "        param_norm = param.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master function for training\n",
    "def train_and_test_transformer(\n",
    "optim_name, # pass as the name containe in a string\n",
    "custom_str = 'Transformer, FullFeatureSet, LongTrain',\n",
    "normalize = 'True',\n",
    "display_time_idx = 500,\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "seed = 42,\n",
    "BS = 32,\n",
    "steps = 1000,\n",
    "clip_grad = np.inf,\n",
    "ntype = 'z',\n",
    "xformer_dim = 128,\n",
    "lr = 1e-5,\n",
    "test_every = 100,\n",
    "lookback = 12\n",
    "):\n",
    "    \n",
    "    # master function to train on data and produce output on test set \n",
    "    model_kwargs = {\n",
    "        'x_size': 6,\n",
    "        'y_size': 1,\n",
    "        'u_size': 2,\n",
    "        's_size': 7,\n",
    "        'lookback': lookback,\n",
    "        'lookahead': lookahead,\n",
    "        'd_model' : xformer_dim,\n",
    "        'e_layers' : 4,\n",
    "        'd_layers' : 4,\n",
    "        'dtype' : dtype\n",
    "    }\n",
    "    model = nn.DataParallel(Transformer(**model_kwargs))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optim = eval(optim_name)(model.parameters(), **{'lr':lr})\n",
    "    loss_fn = MSELoss(ntype)\n",
    "    loss_fn_to_report = MSELoss2(ntype)\n",
    "    \n",
    "    # get and combine datasets\n",
    "    _, train_set, test_set = get_comstock_range(\n",
    "        end_bldg_idx=num_clients,\n",
    "        lookback = lookback,\n",
    "        lookahead = lookahead,\n",
    "        dtype = dtype,\n",
    "        normalize = normalize,\n",
    "        normalize_type=ntype\n",
    "    )\n",
    "    train_set, test_set = ConcatDataset(train_set), ConcatDataset(test_set)\n",
    "    torch.manual_seed(seed)\n",
    "    train_loader = DataLoader(train_set, batch_size=BS, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=8096, shuffle=True)\n",
    "\n",
    "    loss_record, mape_record, norm_record = [], [], []\n",
    "    elapsed = 0\n",
    "    normal_weights(model) # actually initializes to normal, doesnt zero   \n",
    "    for inp, lab in (t:=tqdm(itertools.cycle(train_loader))):\n",
    "    \n",
    "        inp, lab = inp.to(device), lab.to(device)\n",
    "        pred = model(inp)\n",
    "        loss = loss_fn(lab,pred)\n",
    "        loss_to_report = loss_fn_to_report(lab,pred)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        if not np.isinf(clip_grad) and clip_grad > 0:\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        norm_record.append(calculate_gradient_norm(model))\n",
    "        optim.step()\n",
    "        # scheduler.step()    \n",
    "        loss_record.append(loss_to_report.item())\n",
    "        elapsed += 1\n",
    "        \n",
    "        t.set_description(f\"On experiment {custom_str}, step {elapsed}, loss is {loss.item()}.\")\n",
    "        \n",
    "        if elapsed % test_every == 0:\n",
    "            mapes = []\n",
    "            for inp,lab in test_loader:\n",
    "                inp = inp.to(device)\n",
    "                with torch.no_grad():\n",
    "                    pred = model(inp)\n",
    "                mapes.append(mape(lab.to('cpu').numpy(),pred.to('cpu').numpy(),normalization_type=ntype))\n",
    "            metric = np.mean(np.array(mapes))\n",
    "            mape_record.append(metric)\n",
    "            print(f\"On step {elapsed}, MAPE error is {metric} percent.\")\n",
    "            \n",
    "        if elapsed == steps:\n",
    "            break\n",
    "    \n",
    "    # plotting here\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20,4))\n",
    "    # plot losses\n",
    "    loss_record = np.array(loss_record)\n",
    "    axs[0].plot(np.arange(1,loss_record.size+1),np.array(loss_record))\n",
    "    axs[0].set_xlim(1,loss_record.size)\n",
    "    axs[0].set_xlabel('Steps')\n",
    "    axs[0].set_ylabel('MSE Loss')\n",
    "    axs[0].set_title(f'Train Loss')\n",
    "    axs[0].set_yscale('log')\n",
    "    # plot norms\n",
    "    norm_record = np.array(norm_record)\n",
    "    axs[1].plot(np.arange(1,norm_record.size+1),np.array(norm_record))\n",
    "    axs[1].set_xlim(1,norm_record.size)\n",
    "    axs[1].set_xlabel('Steps')\n",
    "    axs[1].set_ylabel('L2 Norm')\n",
    "    axs[1].set_title(f'Gradient Norm clip: {clip_grad}')\n",
    "    axs[1].set_yscale('log')\n",
    "    # plot MAPEs\n",
    "    mape_record = np.array(mape_record)\n",
    "    axs[2].plot(np.arange(1,mape_record.size+1),np.array(mape_record),'ko')\n",
    "    axs[2].set_xlim(1,mape_record.size)\n",
    "    axs[2].set_xlabel(f'Steps x{test_every}')\n",
    "    axs[2].set_ylabel(f'MAPE')\n",
    "    axs[2].set_title(f'Test set.')\n",
    "    # plot the test sets\n",
    "    inputs, outputs = [], []\n",
    "    for idx in range(display_time_idx):\n",
    "        itm = test_set.__getitem__(idx)\n",
    "        inputs.append(itm[0])\n",
    "        outputs.append(itm[1].numpy())\n",
    "    batched_input = torch.stack(inputs).to(dtype).to(device)\n",
    "    with torch.no_grad():   \n",
    "        batched_output = model(batched_input).to('cpu').numpy()\n",
    "    preds = list(batched_output)\n",
    "    plot_gt, plot_pred = [], []\n",
    "    for idx in range(display_time_idx):\n",
    "        minval, maxval = outputs[idx][-1,1], outputs[idx][-1,2]\n",
    "        if ntype == 'minmax':\n",
    "            # minmax\n",
    "            plot_gt.append((outputs[idx][-1,0]-minval)/(maxval-minval))\n",
    "        else:\n",
    "            # z normalization\n",
    "            plot_gt.append((outputs[idx][-1,0]-minval)/maxval)\n",
    "        plot_pred.append(preds[idx][-1,0])\n",
    "    plot_gt, plot_pred = np.array(plot_gt), np.array(plot_pred)\n",
    "    axs[3].plot(np.arange(1,plot_gt.size+1),plot_gt,label='ground truth')\n",
    "    axs[3].plot(np.arange(1,plot_pred.size+1),plot_pred,label='prediction')\n",
    "    axs[3].set_xlim(1,plot_pred.size)\n",
    "    axs[3].set_xlabel('Time index')\n",
    "    axs[3].set_ylabel('kWh')\n",
    "    axs[3].legend()\n",
    "    axs[3].set_title(f'Reconstruction')\n",
    "    \n",
    "    optdict = {\n",
    "        'torch.optim.SGD': 'sgd',\n",
    "        'torch.optim.Adam': 'adam'\n",
    "    }\n",
    "    \n",
    "    plt.suptitle(f'Optim={optdict[optim_name]}, BS={BS}, lr={lr}, clip={clip_grad}')\n",
    "    \n",
    "    plt.savefig(f'/home/sbose/{FOLDER_NAME}/{optdict[optim_name]}_BS_{BS}_lr_{lr}_lookback_{lookback}_transformer_{steps}.pdf',format='pdf',bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # torch.save(model.state_dict(),f'/home/sbose/{FOLDER_NAME}/{optdict[optim_name]}_BS_{BS}_lr_{lr}_clip_{clip_grad}.pth')\n",
    "    np.savez_compressed(f'/home/sbose/{FOLDER_NAME}/{optdict[optim_name]}_BS_{BS}_lr_{lr}_lookback_{lookback}_transformer_{steps}.npz',loss_record=loss_record, mape_record=mape_record,norm_record=norm_record,preds=preds,outputs=outputs)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "On experiment Transformer, step 34, loss is 40.569915771484375.: : 34it [00:05,  6.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/sbose/\u001b[39m\u001b[39m{\u001b[39;00mFOLDER_NAME\u001b[39m}\u001b[39;00m\u001b[39m/done.txt\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m l, b, c, o, lb, stp \u001b[39min\u001b[39;00m configs:\n\u001b[0;32m---> 12\u001b[0m     train_and_test_transformer(\n\u001b[1;32m     13\u001b[0m         o, \u001b[39m# pass as the name containe in a string\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mTransformer\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m         normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     16\u001b[0m         display_time_idx\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m         ntype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mz\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m         xformer_dim\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m         BS \u001b[39m=\u001b[39;49m b,\n\u001b[1;32m     20\u001b[0m         lr \u001b[39m=\u001b[39;49m l,\n\u001b[1;32m     21\u001b[0m         steps\u001b[39m=\u001b[39;49mstp,\n\u001b[1;32m     22\u001b[0m         test_every \u001b[39m=\u001b[39;49m \u001b[39m6000\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m         clip_grad\u001b[39m=\u001b[39;49m c,\n\u001b[1;32m     24\u001b[0m         lookback \u001b[39m=\u001b[39;49m lb\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinished opt=\u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m}\u001b[39;00m\u001b[39m, lr=\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m, BS=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, Clip=\u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     file\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00ml\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 56\u001b[0m, in \u001b[0;36mtrain_and_test_transformer\u001b[0;34m(optim_name, custom_str, normalize, display_time_idx, device, seed, BS, steps, clip_grad, ntype, xformer_dim, lr, test_every, lookback)\u001b[0m\n\u001b[1;32m     54\u001b[0m elapsed \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     55\u001b[0m normal_weights(model) \u001b[39m# actually initializes to normal, doesnt zero   \u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[39mfor\u001b[39;00m inp, lab \u001b[39min\u001b[39;00m (t\u001b[39m:=\u001b[39mtqdm(itertools\u001b[39m.\u001b[39mcycle(train_loader))):\n\u001b[1;32m     58\u001b[0m     inp, lab \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mto(device), lab\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     59\u001b[0m     pred \u001b[39m=\u001b[39m model(inp)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/dataset.py:348\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 348\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/torch/utils/data/dataset.py:411\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 411\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/time-series-forecasting-federation/models/LFDataset.py:132\u001b[0m, in \u001b[0;36mLFDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    130\u001b[0m y_future_copy \u001b[39m=\u001b[39m y_future\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mminmax\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     y_past, y_future, x_past, u_past, u_future, s_past \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(y_past, y_future, x_past, u_past, u_future, s_past)\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mminmax\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    134\u001b[0m         fac_0, fac_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scaler\u001b[39m.\u001b[39mdata_min_\u001b[39m.\u001b[39mitem(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scaler\u001b[39m.\u001b[39mdata_max_\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/time-series-forecasting-federation/models/LFDataset.py:201\u001b[0m, in \u001b[0;36mLFDataset._transform\u001b[0;34m(self, y_past, y_future, x, u_past, u_future, s)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, y_past, y_future, x, u_past, u_future, s):\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scaler\u001b[39m.\u001b[39mtransform(y_past), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scaler\u001b[39m.\u001b[39mtransform(y_future), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_scaler\u001b[39m.\u001b[39mtransform(x), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mu_scaler\u001b[39m.\u001b[39;49mtransform(u_past), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu_scaler\u001b[39m.\u001b[39mtransform(u_future), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms_scaler\u001b[39m.\u001b[39mtransform(s)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1042\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1044\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[0;32m-> 1045\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1046\u001b[0m     X,\n\u001b[1;32m   1047\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1048\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1049\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1050\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m   1051\u001b[0m     force_writeable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1052\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1053\u001b[0m )\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m   1056\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/sklearn/utils/validation.py:118\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m# error message.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(over\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    119\u001b[0m     first_pass_isfinite \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39misfinite(xp\u001b[39m.\u001b[39msum(X))\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/numpy/core/_ufunc_config.py:431\u001b[0m, in \u001b[0;36merrstate.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moldstate \u001b[39m=\u001b[39m seterr(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _Unspecified:\n\u001b[1;32m    433\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moldcall \u001b[39m=\u001b[39m seterrcall(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall)\n",
      "File \u001b[0;32m~/.conda/envs/appfl/lib/python3.10/site-packages/numpy/core/_ufunc_config.py:128\u001b[0m, in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m    122\u001b[0m maskvalue \u001b[39m=\u001b[39m ((_errdict[divide] \u001b[39m<<\u001b[39m SHIFT_DIVIDEBYZERO) \u001b[39m+\u001b[39m\n\u001b[1;32m    123\u001b[0m              (_errdict[over] \u001b[39m<<\u001b[39m SHIFT_OVERFLOW) \u001b[39m+\u001b[39m\n\u001b[1;32m    124\u001b[0m              (_errdict[under] \u001b[39m<<\u001b[39m SHIFT_UNDERFLOW) \u001b[39m+\u001b[39m\n\u001b[1;32m    125\u001b[0m              (_errdict[invalid] \u001b[39m<<\u001b[39m SHIFT_INVALID))\n\u001b[1;32m    127\u001b[0m pyvals[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m maskvalue\n\u001b[0;32m--> 128\u001b[0m umath\u001b[39m.\u001b[39;49mseterrobj(pyvals)\n\u001b[1;32m    129\u001b[0m \u001b[39mreturn\u001b[39;00m old\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    [1e-4, 128, 100, 'torch.optim.SGD', 12, 20000],\n",
    "    [1e-5, 128, 100, 'torch.optim.SGD', 12, 20000],\n",
    "    [1e-4, 128, 100, 'torch.optim.SGD', 12, 100000]\n",
    "]\n",
    "\n",
    "os.makedirs(f'/home/sbose/{FOLDER_NAME}',exist_ok=True)\n",
    "file = open(f'/home/sbose/{FOLDER_NAME}/done.txt','a')\n",
    "\n",
    "for l, b, c, o, lb, stp in configs:\n",
    "    \n",
    "    train_and_test_transformer(\n",
    "        o, # pass as the name containe in a string\n",
    "        'Transformer',\n",
    "        normalize=True,\n",
    "        display_time_idx=250,\n",
    "        ntype='z',\n",
    "        xformer_dim=128,\n",
    "        BS = b,\n",
    "        lr = l,\n",
    "        steps=stp,\n",
    "        test_every = 6000,\n",
    "        clip_grad= c,\n",
    "        lookback = lb\n",
    "    )\n",
    "    print(f\"Finished opt={o}, lr={l}, BS={b}, Clip={c}.\")\n",
    "    file.write(f\"'{o}',{l},{b},{c}\\n\")\n",
    "    \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
