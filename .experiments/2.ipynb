{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  9 05:38:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              61W / 400W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# infer GPU in use\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data file paths\n",
    "filepaths = [f\"/home/sbose/time-series-forecasting-federation/data/NREL{s}dataset.npz\" for s in ['CA','IL','NY']]\n",
    "\n",
    "# in this notebook, we will analyse the data files and try to see how we can make a custom data loader for the same.\n",
    "\n",
    "data_y_s = np.load('/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_data.npz')\n",
    "data_x_u = np.load('/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_weather.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we write some basic code which allows the creation of a dataloader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Union, List, Tuple\n",
    "from itertools import combinations\n",
    "\n",
    "class LFDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_y_s: np.array,\n",
    "        data_x_u: np.array,\n",
    "        lookback: int,\n",
    "        lookahead: int,\n",
    "        client_idx: int,\n",
    "        idx_x: Union[List,Tuple],\n",
    "        idx_u: Union[List,Tuple],\n",
    "        dtype: torch.dtype = torch.float32\n",
    "    ):\n",
    "        \n",
    "        # sanity checks\n",
    "        assert lookback > 0, \"Cannot have non-positive lookback!\"\n",
    "        assert lookahead > 0, \"Cannot have non-positive lookahead!\"\n",
    "        assert client_idx < data_y_s['load'].shape[0], \"Client index exceeds number of clients present.\"\n",
    "        assert len(idx_x)+len(idx_u) == data_x_u['wdata'].shape[0], \"Indices provided do not sum upto the input dimension.\"\n",
    "        assert all(not set(a) & set(b) for a, b in combinations([idx_x, idx_u], 2)), \"All indices are not mutually exclusive.\"\n",
    "        \n",
    "        # save inputs\n",
    "        self.load = data_y_s['load'][client_idx,:]\n",
    "        self.static = data_y_s['static'][client_idx,:]\n",
    "        self.x, self.u = data_x_u['wdata'][idx_x,:], data_x_u['wdata'][idx_u,:]\n",
    "        self.idx_x, idx_u = idx_x, idx_u\n",
    "        self.lookback, self.lookahead = lookback, lookahead\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # max length\n",
    "        self.maxlen = self.load.shape[0] - lookback - lookahead + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.maxlen\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        y_past = torch.tensor(self.load[idx:idx+self.lookback][:,None], dtype=self.dtype)\n",
    "        x_past = torch.tensor(self.x[:,idx:idx+self.lookback].T, dtype=self.dtype)\n",
    "        u_past = torch.tensor(self.u[:,idx:idx+self.lookback].T, dtype=self.dtype)\n",
    "        u_future = torch.tensor(self.u[:,idx+self.lookback:idx+self.lookback+self.lookahead].T, dtype=self.dtype)\n",
    "        s_past = torch.tensor(self.static[None,:].repeat(self.lookback,axis=0), dtype=self.dtype)\n",
    "        y_target = torch.tensor(self.load[idx+self.lookback+self.lookahead-1].reshape((1,)), dtype=self.dtype)\n",
    "        y_all_target = torch.tensor(self.load[idx+self.lookback:idx+self.lookback+self.lookahead][:,None], dtype=self.dtype)\n",
    "        \n",
    "        inp = (y_past,x_past,u_past,s_past,u_future)\n",
    "        lab = (y_target, y_all_target)\n",
    "        \n",
    "        return inp, lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now test out our dataset with california data\n",
    "\n",
    "# create dataset\n",
    "CA_dset = LFDataset(\n",
    "    data_y_s = data_y_s,\n",
    "    data_x_u = data_x_u,\n",
    "    lookback = 8,\n",
    "    lookahead = 4,\n",
    "    client_idx = 0,\n",
    "    idx_x = [0,1],\n",
    "    idx_u = [2,3],\n",
    "    dtype = torch.float32\n",
    ")\n",
    "\n",
    "# load into dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "CA_dataloader = DataLoader(CA_dset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1th item of dataloader, type of a is <class 'list'>, type of b is <class 'list'>\n",
      "Shape of 1th item in a is torch.Size([32, 8, 1]).\n",
      "Shape of 2th item in a is torch.Size([32, 8, 2]).\n",
      "Shape of 3th item in a is torch.Size([32, 8, 2]).\n",
      "Shape of 4th item in a is torch.Size([32, 8, 3]).\n",
      "Shape of 5th item in a is torch.Size([32, 4, 2]).\n",
      "Shape of 1th item in b is torch.Size([32, 1]).\n",
      "Shape of 2th item in b is torch.Size([32, 4, 1]).\n"
     ]
    }
   ],
   "source": [
    "# Test out the shape of the dataloader outputs\n",
    "\n",
    "for cidx, (a,b) in enumerate(CA_dataloader):\n",
    "    print(f\"On {cidx+1}th item of dataloader, type of a is {type(a)}, type of b is {type(b)}\")\n",
    "    for idx,itm in enumerate(a):\n",
    "        print(f\"Shape of {idx+1}th item in a is {itm.shape}.\")\n",
    "    for idx,itm in enumerate(b):\n",
    "        print(f\"Shape of {idx+1}th item in b is {itm.shape}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that relative imports from the git repository can always be found\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/sbose/time-series-forecasting-federation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM output is (32, 1).\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM vanilla version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMFCDecoder import LSTMFCDecoder\n",
    "\n",
    "model = LSTMFCDecoder(\n",
    "    input_size = 8,\n",
    "    hidden_size = 20,\n",
    "    num_layers = 2,\n",
    "    y_size = 1,\n",
    "    fcnn_sizes = (160,10,10,1),\n",
    "    activation = nn.ReLU,\n",
    "    lookback = 8,\n",
    "    lookahead = 4,\n",
    "    dtype = torch.float32\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of LSTM output is {tuple(w.shape)}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM output is (32, 4, 1).\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM autoregressive version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "\n",
    "model = LSTMAR(\n",
    "    input_size = 8,\n",
    "    u_size = 2,\n",
    "    hidden_size = 20,\n",
    "    num_layers = 2,\n",
    "    y_size = 1,\n",
    "    fcnn_sizes = (20,10,10,1),\n",
    "    activation = nn.ReLU,\n",
    "    lookahead = 4,\n",
    "    dtype = torch.float32\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of LSTM output is {tuple(w.shape)}.\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
