{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 17 12:17:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   20C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   49C    P0             315W / 400W |   7109MiB / 40960MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   50C    P0             278W / 400W |   7109MiB / 40960MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   47C    P0             287W / 400W |   7109MiB / 40960MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              53W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   65C    P0             343W / 400W |   7109MiB / 40960MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    1   N/A  N/A   3648627      C   ./three_pt_production                      7100MiB |\n",
      "|    2   N/A  N/A   3648835      C   ./three_pt_production                      7100MiB |\n",
      "|    3   N/A  N/A   3648908      C   ./three_pt_production                      7100MiB |\n",
      "|    6   N/A  N/A   3648979      C   ./three_pt_production                      7100MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# infer GPU in use\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# in this notebook, we will analyse the data files and try to see how we can make a custom data loader for the same.\n",
    "\n",
    "data_y_s = np.load('/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_data.npz')\n",
    "data_x_u = np.load('/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_weather.npz')\n",
    "\n",
    "# function to calculate model sizes\n",
    "\n",
    "def model_size_in_mb(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    size_in_mb = param_size / (1024 ** 2)\n",
    "    return size_in_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we write some basic code which allows the creation of a dataloader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Union, List, Tuple\n",
    "from itertools import combinations\n",
    "\n",
    "# class LFDataset(Dataset):\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         data_y_s: np.array,\n",
    "#         data_x_u: np.array,\n",
    "#         lookback: int,\n",
    "#         lookahead: int,\n",
    "#         client_idx: int,\n",
    "#         idx_x: Union[List,Tuple],\n",
    "#         idx_u: Union[List,Tuple],\n",
    "#         dtype: torch.dtype = torch.float32\n",
    "#     ):\n",
    "        \n",
    "#         # sanity checks\n",
    "#         assert lookback > 0, \"Cannot have non-positive lookback!\"\n",
    "#         assert lookahead > 0, \"Cannot have non-positive lookahead!\"\n",
    "#         assert client_idx < data_y_s['load'].shape[0], \"Client index exceeds number of clients present.\"\n",
    "#         assert len(idx_x)+len(idx_u) == data_x_u['wdata'].shape[0], \"Indices provided do not sum upto the input dimension.\"\n",
    "#         assert all(not set(a) & set(b) for a, b in combinations([idx_x, idx_u], 2)), \"All indices are not mutually exclusive.\"\n",
    "        \n",
    "#         # save inputs\n",
    "#         self.load = data_y_s['load'][client_idx,:]\n",
    "#         self.static = data_y_s['static'][client_idx,:]\n",
    "#         self.x, self.u = data_x_u['wdata'][idx_x,:], data_x_u['wdata'][idx_u,:]\n",
    "#         self.idx_x, idx_u = idx_x, idx_u\n",
    "#         self.lookback, self.lookahead = lookback, lookahead\n",
    "#         self.dtype = dtype\n",
    "        \n",
    "#         # max length\n",
    "#         self.maxlen = self.load.shape[0] - lookback - lookahead + 1\n",
    "        \n",
    "#     def __len__(self):\n",
    "        \n",
    "#         return self.maxlen\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         y_past = torch.tensor(self.load[idx:idx+self.lookback][:,None], dtype=self.dtype)\n",
    "#         x_past = torch.tensor(self.x[:,idx:idx+self.lookback].T, dtype=self.dtype)\n",
    "#         u_past = torch.tensor(self.u[:,idx:idx+self.lookback].T, dtype=self.dtype)\n",
    "#         u_future = torch.tensor(self.u[:,idx+self.lookback:idx+self.lookback+self.lookahead].T, dtype=self.dtype)\n",
    "#         s_past = torch.tensor(self.static[None,:].repeat(self.lookback,axis=0), dtype=self.dtype)\n",
    "#         y_target = torch.tensor(self.load[idx+self.lookback+self.lookahead-1].reshape((1,)), dtype=self.dtype)\n",
    "#         y_all_target = torch.tensor(self.load[idx+self.lookback:idx+self.lookback+self.lookahead][:,None], dtype=self.dtype)\n",
    "        \n",
    "#         inp = (y_past,x_past,u_past,s_past,u_future)\n",
    "#         lab = (y_target, y_all_target)\n",
    "        \n",
    "#         return inp, lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now test out our dataset with california data\n",
    "\n",
    "sys.path.insert(0,'/home/sbose/time-series-forecasting-federation')\n",
    "from models.LFDataset import LFDataset\n",
    "\n",
    "# create dataset\n",
    "CA_dset = LFDataset(\n",
    "    data_y_s = data_y_s,\n",
    "    data_x_u = data_x_u,\n",
    "    lookback = 8,\n",
    "    lookahead = 4,\n",
    "    client_idx = 0,\n",
    "    idx_x = [0,1,2,3,4,5],\n",
    "    idx_u = [6,7],\n",
    "    dtype = torch.float32\n",
    ")\n",
    "\n",
    "# load into dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "CA_dataloader = DataLoader(CA_dset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1th item of dataloader, type of a is <class 'models.LFDataset.TensorList'>, type of b is <class 'models.LFDataset.TensorList'>\n",
      "Shape of 1th item in a is torch.Size([32, 8, 1]).\n",
      "Shape of 2th item in a is torch.Size([32, 8, 6]).\n",
      "Shape of 3th item in a is torch.Size([32, 8, 2]).\n",
      "Shape of 4th item in a is torch.Size([32, 8, 7]).\n",
      "Shape of 5th item in a is torch.Size([32, 4, 2]).\n",
      "Shape of 6th item in a is torch.Size([32, 4, 1]).\n",
      "Shape of 1th item in b is torch.Size([32, 1]).\n",
      "Shape of 2th item in b is torch.Size([32, 4, 1]).\n"
     ]
    }
   ],
   "source": [
    "# Test out the shape of the dataloader outputs\n",
    "\n",
    "for cidx, (a,b) in enumerate(CA_dataloader):\n",
    "    print(f\"On {cidx+1}th item of dataloader, type of a is {type(a)}, type of b is {type(b)}\")\n",
    "    for idx,itm in enumerate(a):\n",
    "        print(f\"Shape of {idx+1}th item in a is {itm.shape}.\")\n",
    "    for idx,itm in enumerate(b):\n",
    "        print(f\"Shape of {idx+1}th item in b is {itm.shape}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that relative imports from the git repository can always be found\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/sbose/time-series-forecasting-federation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM output is (32, 1).\n",
      "LSTM FCNN head: 0.07667922973632812 MB\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM vanilla version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMFCDecoder import LSTMFCDecoder\n",
    "\n",
    "model = LSTMFCDecoder(\n",
    "    input_size = 16,\n",
    "    hidden_size = 20,\n",
    "    num_layers = 2,\n",
    "    y_size = 1,\n",
    "    fcnn_sizes = (160,80,10,1),\n",
    "    activation = nn.ReLU,\n",
    "    lookback = 8,\n",
    "    lookahead = 4,\n",
    "    dtype = torch.float32\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of LSTM output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"LSTM FCNN head: {model_size_in_mb(model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM output is (32, 4, 1).\n",
      "LSTM AR: 0.045818328857421875 MB\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM autoregressive version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "\n",
    "model = LSTMAR(\n",
    "    input_size = 16, # x + u + y + s\n",
    "    u_size = 2, # u\n",
    "    hidden_size = 20,\n",
    "    num_layers = 2,\n",
    "    y_size = 1, # y\n",
    "    fcnn_sizes = (20,10,10,1),\n",
    "    activation = nn.ReLU,\n",
    "    lookahead = 4,\n",
    "    dtype = torch.float32\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of LSTM output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"LSTM AR: {model_size_in_mb(model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # we first map data types here\n",
    "\n",
    "# dtype_map = {\n",
    "#     torch.float32: np.float32,\n",
    "#     torch.float: np.float32,\n",
    "#     torch.float64: np.float64,\n",
    "#     torch.double: np.float64,\n",
    "#     torch.float16: np.float16,\n",
    "#     torch.half: np.float16,\n",
    "#     torch.uint8: np.uint8,\n",
    "#     torch.int8: np.int8,\n",
    "#     torch.int16: np.int16,\n",
    "#     torch.short: np.int16,\n",
    "#     torch.int32: np.int32,\n",
    "#     torch.int: np.int32,\n",
    "#     torch.int64: np.int64,\n",
    "#     torch.long: np.int64,\n",
    "#     torch.bool: np.bool_,\n",
    "#     torch.complex64: np.complex64,\n",
    "#     torch.complex128: np.complex128\n",
    "# }\n",
    "\n",
    "# # some functions to flatten and unflatten state dicts to.from numpy vectors\n",
    "\n",
    "# def state_dict_to_vector(state_dict):\n",
    "#     param_vector = [param.detach().cpu().numpy().astype(dtype_map[param.dtype]).flatten() for param in state_dict.values()]\n",
    "#     return np.concatenate(param_vector)\n",
    "\n",
    "# def load_vector_to_state_dict(vector, reference_state_dict):\n",
    "#     pointer = 0\n",
    "#     with torch.no_grad():\n",
    "#         for param in reference_state_dict.values():\n",
    "#             num_param = param.numel()\n",
    "#             param.copy_(torch.from_numpy(vector[pointer:pointer + num_param]).view_as(param).to(param.dtype))\n",
    "#             pointer += num_param\n",
    "\n",
    "# def gradients_to_vector(state_dict, dtype_map):\n",
    "#     grad_vector = [(param.grad.detach().cpu().numpy().astype(dtype_map[param.dtype]).flatten() if param.grad is not None\n",
    "#                     else np.zeros(param.numel(), dtype=dtype_map[param.dtype]))\n",
    "#                    for param in state_dict.values()]\n",
    "#     return np.concatenate(grad_vector)\n",
    "\n",
    "# def load_vector_to_gradients(vector, reference_state_dict, dtype_map):\n",
    "#     pointer = 0\n",
    "#     with torch.no_grad():\n",
    "#         for param in reference_state_dict.values():\n",
    "#             num_elements = param.numel()\n",
    "#             part_vector = vector[pointer:pointer + num_elements]\n",
    "#             grad_tensor = torch.from_numpy(part_vector).view_as(param).to(dtype=dtype_map[param.dtype])\n",
    "#             if param.grad is not None:\n",
    "#                 param.grad.copy_(grad_tensor)\n",
    "#             else:\n",
    "#                 param.grad = grad_tensor.to(param.device)\n",
    "#             pointer += num_elements\n",
    "            \n",
    "# # we now load the name of all the files\n",
    "\n",
    "# base_grouped_dir = '/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped'\n",
    "# suffix = '_data.npz'\n",
    "# filenames = []\n",
    "\n",
    "# for filename in os.listdir(base_grouped_dir):\n",
    "#     if filename.endswith(suffix):\n",
    "#         # Extract the leading characters and add them to the list\n",
    "#         filenames.append(filename[:-len(suffix)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM output is (32, 4, 1).\n",
      "DARNN: 0.0562744140625 MB\n"
     ]
    }
   ],
   "source": [
    "# test out DARNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.DARNN.DARNN import DARNN\n",
    "\n",
    "model = DARNN(\n",
    "        x_size = 6,\n",
    "        y_size = 1,\n",
    "        u_size = 2,\n",
    "        s_size = 7,\n",
    "        encoder_hidden_size = 20,\n",
    "        decoder_hidden_size = 20,\n",
    "        encoder_num_layers = 2,\n",
    "        decoder_num_layers = 2,\n",
    "        lookback = 8,\n",
    "        lookahead = 4,\n",
    "        dtype = torch.float32\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of LSTM output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"DARNN: {model_size_in_mb(model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformer AR output is (32, 4, 1).\n",
      "Shape of Transformer AR output in test mode is (32, 4, 1).\n",
      "Transformer AR: 4.423839569091797 MB\n"
     ]
    }
   ],
   "source": [
    "# test out DARNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.TRANSFORMER.TransformerAR import TransformerAR\n",
    "\n",
    "model = TransformerAR(\n",
    "        x_size = 6,\n",
    "        y_size = 1,\n",
    "        u_size = 2,\n",
    "        s_size = 7\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    wtest = model(a,mode='test')\n",
    "    print(f\"Shape of Transformer AR output is {tuple(w.shape)}.\")\n",
    "    print(f\"Shape of Transformer AR output in test mode is {tuple(wtest.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"Transformer AR: {model_size_in_mb(model)} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
