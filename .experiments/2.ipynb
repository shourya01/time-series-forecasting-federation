{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 22 18:03:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              54W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   21C    P0              52W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              52W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   54C    P0             263W / 400W |  26781MiB / 40960MiB |     95%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             128W / 400W |   7869MiB / 40960MiB |     35%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             139W / 400W |   5853MiB / 40960MiB |     35%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    4   N/A  N/A   1326064      C   ...xu/ENVS/allegroJan31/bin/python3.10    26772MiB |\n",
      "|    5   N/A  N/A   1719078      C   python                                     7860MiB |\n",
      "|    6   N/A  N/A   1586486      C   python                                     5844MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# infer GPU in use\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# in this notebook, we will analyse the data files and try to see how we can make a custom data loader for the same.\n",
    "\n",
    "data_y_s = np.load('/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_data.npz')\n",
    "data_x_u = np.load('/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_weather.npz')\n",
    "\n",
    "# function to calculate model sizes\n",
    "\n",
    "def model_size_in_mb(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    size_in_mb = param_size / (1024 ** 2)\n",
    "    return size_in_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now test out our dataset with california data\n",
    "\n",
    "sys.path.insert(0,'/home/sbose/time-series-forecasting-federation')\n",
    "from models.LFDataset import LFDataset\n",
    "\n",
    "# create dataset\n",
    "CA_dset = LFDataset(\n",
    "    data_y_s = data_y_s,\n",
    "    data_x_u = data_x_u,\n",
    "    lookback = 8,\n",
    "    lookahead = 4,\n",
    "    client_idx = 0,\n",
    "    idx_x = [0,1,2,3,4,5],\n",
    "    idx_u = [6,7],\n",
    "    dtype = torch.float32\n",
    ")\n",
    "\n",
    "# load into dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "CA_dataloader = DataLoader(CA_dset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1th item of dataloader, type of a is <class 'models.LFDataset.TensorList'>, type of b is <class 'models.LFDataset.TensorList'>\n",
      "Shape of 1th item in a is torch.Size([32, 8, 1]).\n",
      "Shape of 2th item in a is torch.Size([32, 8, 6]).\n",
      "Shape of 3th item in a is torch.Size([32, 8, 2]).\n",
      "Shape of 4th item in a is torch.Size([32, 8, 7]).\n",
      "Shape of 5th item in a is torch.Size([32, 4, 2]).\n",
      "Shape of 6th item in a is torch.Size([32, 4, 1]).\n",
      "Shape of 1th item in b is torch.Size([32, 1]).\n",
      "Shape of 2th item in b is torch.Size([32, 4, 1]).\n"
     ]
    }
   ],
   "source": [
    "# Test out the shape of the dataloader outputs\n",
    "\n",
    "for cidx, (a,b) in enumerate(CA_dataloader):\n",
    "    print(f\"On {cidx+1}th item of dataloader, type of a is {type(a)}, type of b is {type(b)}\")\n",
    "    for idx,itm in enumerate(a):\n",
    "        print(f\"Shape of {idx+1}th item in a is {itm.shape}.\")\n",
    "    for idx,itm in enumerate(b):\n",
    "        print(f\"Shape of {idx+1}th item in b is {itm.shape}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that relative imports from the git repository can always be found\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/sbose/time-series-forecasting-federation')\n",
    "\n",
    "# kwargs for all models\n",
    "model_kwargs = {\n",
    "    'x_size': 6,\n",
    "    'y_size': 1,\n",
    "    'u_size': 2,\n",
    "    's_size': 7,\n",
    "    'lookback': 8,\n",
    "    'lookahead': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM FCNN output is (32, 1).\n",
      "LSTM FCNN head: 0.07667922973632812 MB\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM vanilla version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMFCDecoder import LSTMFCDecoder\n",
    "\n",
    "model = LSTMFCDecoder(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of LSTM FCNN output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"LSTM FCNN head: {model_size_in_mb(model)} MB\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LSTMFCDecoder(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM AR output is (32, 4, 1).\n",
      "LSTM AR: 0.045818328857421875 MB\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM autoregressive version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "\n",
    "model = LSTMAR(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of LSTM AR output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"LSTM AR: {model_size_in_mb(model)} MB\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LSTMAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DARNN output is (32, 4, 1).\n",
      "DARNN: 0.0562744140625 MB\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out DARNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.DARNN.DARNN import DARNN\n",
    "\n",
    "model = DARNN(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of DARNN output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"DARNN: {model_size_in_mb(model)} MB\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = DARNN(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformer AR output is (32, 4, 1).\n",
      "Shape of Transformer AR output in test mode is (32, 4, 1).\n",
      "Transformer AR: 0.8964958190917969 MB\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Transformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.TRANSFORMER.TransformerAR import TransformerAR\n",
    "\n",
    "model = TransformerAR(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    wtest = model(a,mode='test')\n",
    "    print(f\"Shape of Transformer AR output is {tuple(w.shape)}.\")\n",
    "    print(f\"Shape of Transformer AR output in test mode is {tuple(wtest.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"Transformer AR: {model_size_in_mb(model)} MB\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = TransformerAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Logtrans AR output is (32, 4, 1).\n",
      "Logtrans AR: 0.7866249084472656 MB\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Logtrans\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LOGTRANS.LogTransAR import LogTransAR\n",
    "model = LogTransAR(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of Logtrans AR output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"Logtrans AR: {model_size_in_mb(model)} MB\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LogTransAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Informer output is (32, 4, 1).\n",
      "Informer: 1.4672889709472656 MB\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Informer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.INFORMER.Informer import Informer\n",
    "model = Informer(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of Informer output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"Informer: {model_size_in_mb(model)} MB\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Informer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Autoformer output is (32, 4, 1).\n",
      "Autoformer: 1.4076271057128906 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbose/time-series-forecasting-federation/models/AUTOFORMER/Autoformer.py:23: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/EmptyTensor.cpp:41.)\n",
      "  rfft_output = torch.fft.rfft(padded_tensor, dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Autoformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.AUTOFORMER.Autoformer import Autoformer\n",
    "model = Autoformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of Autoformer output is {tuple(w.shape)}.\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"Autoformer: {model_size_in_mb(model)} MB\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Autoformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
