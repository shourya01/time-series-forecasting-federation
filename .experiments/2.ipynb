{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 30 15:10:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              58W / 400W |   2432MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3650329      C   /home/sbose/.conda/envs/FL/bin/python      2424MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# infer GPU in use\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "computer = 'argonne' # 'ucsc' or 'argonne'\n",
    "\n",
    "# configure filepaths according to which computer we are on\n",
    "\n",
    "if computer == 'ucsc':\n",
    "    data_file = '/home/exx/shourya/comstock_data/G4601010_data.npz'\n",
    "    weather_file = '/home/exx/shourya/comstock_data/G4601010_weather.npz'\n",
    "    module_path = '/home/exx/shourya/time-series-forecasting-federation'\n",
    "else:\n",
    "    data_file = '/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_data.npz'\n",
    "    weather_file = '/lcrc/project/NEXTGENOPT/NREL_COMSTOCK_DATA/grouped/G4601010_weather.npz'\n",
    "    module_path = '/home/sbose/time-series-forecasting-federation'\n",
    "\n",
    "# in this notebook, we will analyse the data files and try to see how we can make a custom data loader for the same.\n",
    "\n",
    "data_y_s = np.load(data_file)\n",
    "data_x_u = np.load(weather_file)\n",
    "\n",
    "# function to calculate model sizes\n",
    "\n",
    "def model_size_in_mb(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    size_in_mb = param_size / (1024 ** 2)\n",
    "    return f'{size_in_mb:.6f} MB'\n",
    "\n",
    "# function to save model to disk and calculate its size\n",
    "\n",
    "def save_and_measure_model(model):\n",
    "    # Save state_dict\n",
    "    torch.save(model.state_dict(), 'model_state.pth')\n",
    "\n",
    "    # Measure file size\n",
    "    file_size = os.path.getsize('model_state.pth') / (1024 * 1024)\n",
    "\n",
    "    # Delete the file\n",
    "    os.remove('model_state.pth')\n",
    "\n",
    "    return f'{file_size:.6f} MB'\n",
    "    \n",
    "# base data type\n",
    "base_type = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now test out our dataset with california data\n",
    "\n",
    "sys.path.insert(0,module_path)\n",
    "from models.LFDataset import LFDataset\n",
    "\n",
    "# create dataset\n",
    "CA_dset = LFDataset(\n",
    "    data_y_s = data_y_s,\n",
    "    data_x_u = data_x_u,\n",
    "    lookback = 12,\n",
    "    lookahead = 4,\n",
    "    client_idx = 0,\n",
    "    idx_x = [0,1,2,3,4,5],\n",
    "    idx_u = [6,7],\n",
    "    dtype = base_type\n",
    ")\n",
    "\n",
    "# load into dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "CA_dataloader = DataLoader(CA_dset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1th item of dataloader, type of a is <class 'models.LFDataset.TensorList'>, type of b is <class 'models.LFDataset.TensorList'>\n",
      "Shape of 1th item in a is torch.Size([32, 12, 1]).\n",
      "Shape of 2th item in a is torch.Size([32, 12, 6]).\n",
      "Shape of 3th item in a is torch.Size([32, 12, 2]).\n",
      "Shape of 4th item in a is torch.Size([32, 12, 7]).\n",
      "Shape of 5th item in a is torch.Size([32, 4, 2]).\n",
      "Shape of 6th item in a is torch.Size([32, 4, 1]).\n",
      "Shape of 1th item in b is torch.Size([32, 1]).\n",
      "Shape of 2th item in b is torch.Size([32, 4, 1]).\n"
     ]
    }
   ],
   "source": [
    "# Test out the shape of the dataloader outputs\n",
    "\n",
    "for cidx, (a,b) in enumerate(CA_dataloader):\n",
    "    print(f\"On {cidx+1}th item of dataloader, type of a is {type(a)}, type of b is {type(b)}\")\n",
    "    for idx,itm in enumerate(a):\n",
    "        print(f\"Shape of {idx+1}th item in a is {itm.shape}.\")\n",
    "    for idx,itm in enumerate(b):\n",
    "        print(f\"Shape of {idx+1}th item in b is {itm.shape}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that relative imports from the git repository can always be found\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,module_path)\n",
    "\n",
    "# kwargs for all models\n",
    "model_kwargs = {\n",
    "    'x_size': 6,\n",
    "    'y_size': 1,\n",
    "    'u_size': 2,\n",
    "    's_size': 7,\n",
    "    'lookback': 12,\n",
    "    'lookahead': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM FCNN head output is (32, 1)\n",
      "\n",
      "LSTM FCNN head, torch.float32, theoretical: 0.101093 MB\n",
      "LSTM FCNN head, torch.float32, state_dict on disk: 0.101093 MB\n",
      "LSTM FCNN head, torch.float16, theoretical: 0.050547 MB\n",
      "LSTM FCNN head, torch.float16, state_dict on disk: 0.050547 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM vanilla version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMFCDecoder import LSTMFCDecoder\n",
    "\n",
    "model = LSTMFCDecoder(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'LSTM FCNN head'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LSTMFCDecoder(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LSTM AR output is (32, 4, 1)\n",
      "\n",
      "LSTM AR, torch.float32, theoretical: 0.045818 MB\n",
      "LSTM AR, torch.float32, state_dict on disk: 0.045818 MB\n",
      "LSTM AR, torch.float16, theoretical: 0.022909 MB\n",
      "LSTM AR, torch.float16, state_dict on disk: 0.022909 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out LSTM autoregressive version\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LSTM.LSTMAR import LSTMAR\n",
    "\n",
    "model = LSTMAR(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'LSTM AR'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LSTMAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DARNN output is (32, 4, 1)\n",
      "\n",
      "DARNN, torch.float32, theoretical: 0.057205 MB\n",
      "DARNN, torch.float32, state_dict on disk: 0.057205 MB\n",
      "DARNN, torch.float16, theoretical: 0.028603 MB\n",
      "DARNN, torch.float16, state_dict on disk: 0.028603 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out DARNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.DARNN.DARNN import DARNN\n",
    "\n",
    "model = DARNN(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'DARNN'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = DARNN(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformer AR output is (32, 4, 1)\n",
      "\n",
      "Transformer AR, torch.float32, theoretical: 0.896496 MB\n",
      "Transformer AR, torch.float32, state_dict on disk: 0.896496 MB\n",
      "Transformer AR, torch.float16, theoretical: 0.448248 MB\n",
      "Transformer AR, torch.float16, state_dict on disk: 0.448248 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Transformer AR\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.TRANSFORMER.TransformerAR import TransformerAR\n",
    "\n",
    "model = TransformerAR(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Transformer AR'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = TransformerAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Transformer output is (32, 4, 1)\n",
      "\n",
      "Transformer, torch.float32, theoretical: 1.419682 MB\n",
      "Transformer, torch.float32, state_dict on disk: 1.419682 MB\n",
      "Transformer, torch.float16, theoretical: 0.448248 MB\n",
      "Transformer, torch.float16, state_dict on disk: 0.448248 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Transformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.TRANSFORMER.Transformer import Transformer\n",
    "\n",
    "model = Transformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Transformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = TransformerAR(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of LogTrans output is (32, 4, 1)\n",
      "\n",
      "LogTrans, torch.float32, theoretical: 1.419682 MB\n",
      "LogTrans, torch.float32, state_dict on disk: 1.419682 MB\n",
      "LogTrans, torch.float16, theoretical: 0.709841 MB\n",
      "LogTrans, torch.float16, state_dict on disk: 0.709841 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Logtrans\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.LOGTRANS.LogTrans import LogTrans\n",
    "model = LogTrans(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'LogTrans'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = LogTrans(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Informer output is (32, 4, 1)\n",
      "\n",
      "Informer, torch.float32, theoretical: 1.467289 MB\n",
      "Informer, torch.float32, state_dict on disk: 1.467289 MB\n",
      "Informer, torch.float16, theoretical: 0.733644 MB\n",
      "Informer, torch.float16, state_dict on disk: 0.733644 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Informer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.INFORMER.Informer import Informer\n",
    "model = Informer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Informer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Informer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Autoformer output is (32, 4, 1)\n",
      "\n",
      "Autoformer, torch.float32, theoretical: 1.407627 MB\n",
      "Autoformer, torch.float32, state_dict on disk: 1.407627 MB\n",
      "\n",
      "NOTICE: Autoformer does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Autoformer, torch.float64, theoretical: 2.815254 MB\n",
      "Autoformer, torch.float64, state_dict on disk: 2.815254 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Autoformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.AUTOFORMER.Autoformer import Autoformer\n",
    "model = Autoformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Autoformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = Autoformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Fedformer Wavelet output is (32, 4, 1)\n",
      "\n",
      "Fedformer Wavelet, torch.float32, theoretical: 770.922871 MB\n",
      "Fedformer Wavelet, torch.float32, state_dict on disk: 770.922871 MB\n",
      "\n",
      "NOTICE: Fedformer Wavelet does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Fedformer Wavelet, torch.float64, theoretical: 1541.845467 MB\n",
      "Fedformer Wavelet, torch.float64, state_dict on disk: 1541.845467 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Fedformer Wavelet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.FEDFORMER.FedformerWavelet import FedformerWavelet\n",
    "model = FedformerWavelet(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Fedformer Wavelet'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = FedformerWavelet(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Fedformer Fourier output is (32, 4, 1)\n",
      "\n",
      "Fedformer Fourier, torch.float32, theoretical: 1.468189 MB\n",
      "Fedformer Fourier, torch.float32, state_dict on disk: 1.468189 MB\n",
      "\n",
      "NOTICE: Fedformer Fourier does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\n",
      "\n",
      "Fedformer Fourier, torch.float64, theoretical: 2.936378 MB\n",
      "Fedformer Fourier, torch.float64, state_dict on disk: 2.936378 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float64.\n",
      "Output dtype is device is torch.float64.\n"
     ]
    }
   ],
   "source": [
    "# test out Fedformer Fourier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.FEDFORMER.FedformerFourier import FedformerFourier\n",
    "model = FedformerFourier(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Fedformer Fourier'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float64, 'cuda'\n",
    "print(f\"\\nNOTICE: {model_name} does not support torch.float16 due to torch.fft.rfft not working with Half for all input tensor shapes.\\n\")\n",
    "model2 = FedformerFourier(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Crossformer output is (32, 4, 1)\n",
      "\n",
      "Crossformer, torch.float32, theoretical: 3.905296 MB\n",
      "Crossformer, torch.float32, state_dict on disk: 3.905296 MB\n",
      "Crossformer, torch.float16, theoretical: 1.952648 MB\n",
      "Crossformer, torch.float16, state_dict on disk: 1.952648 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Crossformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.CROSSFORMER.Crossformer import Crossformer\n",
    "model = Crossformer(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'Crossformer'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = Crossformer(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mLSTM output is (32, 4, 1)\n",
      "\n",
      "mLSTM, torch.float32, theoretical: 1.063274 MB\n",
      "mLSTM, torch.float32, state_dict on disk: 1.063274 MB\n",
      "mLSTM, torch.float16, theoretical: 0.531637 MB\n",
      "mLSTM, torch.float16, state_dict on disk: 0.531637 MB\n",
      "\n",
      "----\n",
      "TESTING IO DATA TYPES FOR torch.float{16|64}.\n",
      "----\n",
      "\n",
      "Input dtype is torch.float16.\n",
      "Output dtype is device is torch.float16.\n"
     ]
    }
   ],
   "source": [
    "# test out Crossformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.XLSTM.mLSTM import mLSTM\n",
    "model = mLSTM(\n",
    "    **model_kwargs\n",
    ")\n",
    "model_name = 'mLSTM'\n",
    "\n",
    "# evaluate the model\n",
    "for a,b in CA_dataloader:\n",
    "    w = model(a)\n",
    "    print(f\"Shape of {model_name} output is {tuple(w.shape)}\\n\")\n",
    "    break\n",
    "\n",
    "# print model size\n",
    "print(f\"{model_name}, {base_type}, theoretical: {model_size_in_mb(model)}\")\n",
    "print(f\"{model_name}, {base_type}, state_dict on disk: {model_size_in_mb(model)}\")\n",
    "\n",
    "# create model of a different dtype\n",
    "dtype, device = torch.float16, 'cuda'\n",
    "model2 = mLSTM(\n",
    "    **model_kwargs,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(f\"{model_name}, {dtype}, theoretical: {model_size_in_mb(model2)}\")\n",
    "print(f\"{model_name}, {dtype}, state_dict on disk: {model_size_in_mb(model2)}\")\n",
    "\n",
    "# evaluate the model\n",
    "print(\"\\n----\\nTESTING IO DATA TYPES FOR torch.float{16|64}.\\n----\\n\")\n",
    "for a,b in CA_dataloader:\n",
    "    a,b = a.to(dtype).to(device), b.to(dtype).to(device)\n",
    "    w = model2(a)\n",
    "    print(f\"Input dtype is {dtype}.\")\n",
    "    print(f\"Output dtype is device is {w.dtype}.\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
